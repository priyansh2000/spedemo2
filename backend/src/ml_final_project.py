# -*- coding: utf-8 -*-
"""ML_Final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QHw2YDEp8FnFMi7d6vJ587Q8qyXur-5R
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import os
import pickle

dataset_path = os.getenv('DATASET_PATH', '/home/prabhav/SPE_Project/backend/data/raw')
print("Files in dataset folder:", os.listdir(dataset_path))
df = pd.read_csv(os.path.join(dataset_path, 'Liver Patient Dataset (LPD)_train.csv'), encoding='latin1')
print(df.head())
print(df.head())
df.info()

df.dropna(inplace=True)

df['Result'] = df['Result'].map({1: 1, 2: 0})

df['Gender of the patient'] = df['Gender of the patient'].map({'Male': 0, 'Female': 1})


numerical_features = ['Age of the patient', 'Total Bilirubin', 'Direct Bilirubin', '\u00a0Alkphos Alkaline Phosphotase',
                      '\u00a0Sgpt Alamine Aminotransferase', 'Sgot Aspartate Aminotransferase', 'Total Protiens',
                      '\u00a0ALB Albumin', 'A/G Ratio Albumin and Globulin Ratio','Gender of the patient']


X = df[numerical_features]
y = df['Result']

split_ratio = 0.8
split_index = int(len(X) * split_ratio)

X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]

train_df = X_train.copy()
train_df['Result'] = y_train.values

test_df = X_test.copy()
test_df['Result'] = y_test.values

"""Here we are removing all the rows which has a null value, this is done to avoid incorrect assumptions about the missing values.

Other alternatives that can be done are:

1. Group-Specific Imputation
2. Predictive Imputation
"""

train_df.head()

train_df.describe()

train_df.shape

train_df.info()

category_counts = train_df['Gender of the patient'].value_counts()
print(category_counts)

for column in train_df.columns:
    print(f"Unique values in {column}:")
    print(train_df[column].unique())

test_df.info()

"""OUTLIERS FINDING"""

def find_outliers_iqr(data):

    Q1 = np.percentile(data, 25)
    Q3 = np.percentile(data, 75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    return outliers.tolist()


numeric_df = train_df.select_dtypes(include=['number'])
for column in numeric_df.columns:
    outliers = find_outliers_iqr(train_df[column])
    if outliers:
        print(f"Outliers in {column}: {outliers}")
    else:
        print(f"No outliers found in {column}")

"""This is done to detect and handle outliers in a dataset by using the Interquartile Range method."""

train_df.describe()

"""OUTLIERS REMOVAL BY IMPUTING"""

def impute_outliers_with_median_no_lib(data):

    data_sorted = sorted(data)
    n = len(data)

    if n % 2 == 0:
        median = (data_sorted[n // 2 - 1] + data_sorted[n // 2]) / 2
    else:
        median = data_sorted[n // 2]

    q1_index = (n + 1) // 4
    q3_index = 3 * (n + 1) // 4

    if q1_index % 1 != 0:
        q1 = (data_sorted[int(q1_index) - 1] + data_sorted[int(q1_index)]) / 2
    else:
        q1 = data_sorted[int(q1_index) - 1]

    if q3_index % 1 != 0:
        q3 = (data_sorted[int(q3_index) - 1] + data_sorted[int(q3_index)]) / 2
    else:
        q3 = data_sorted[int(q3_index) - 1]

    iqr = q3 - q1
    lower_bound = q1 - 1.5 * iqr
    upper_bound = q3 + 1.5 * iqr

    imputed_data = [x if lower_bound <= x <= upper_bound else median for x in data]

    return imputed_data

numeric_df = train_df.select_dtypes(include=['number'])

for column in numeric_df.columns:
    train_df[column] = impute_outliers_with_median_no_lib(train_df[column].tolist())


numeric_df = train_df.select_dtypes(include=['number'])

train_df.describe()

for column in train_df.columns:
    print(f"Unique values in {column}:")
    print(df[column].unique())

train_df.info()

train_df.describe()







def zscore_scaling(data, mean, std):
    return (data - mean) / std

train_mean = train_df[numerical_features].mean()
train_std = train_df[numerical_features].std()

train_df[numerical_features] = zscore_scaling(train_df[numerical_features], train_mean, train_std)
test_df[numerical_features] = zscore_scaling(test_df[numerical_features], train_mean, train_std)

"""PCA"""

numeric_df_train = train_df.select_dtypes(include=['number'])
data_train = numeric_df_train.to_numpy()
cov_matrix_train = np.cov(data_train, rowvar=False)
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix_train)

sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]

explained_variance_ratio = eigenvalues / np.sum(eigenvalues)

cumulative_variance = np.cumsum(explained_variance_ratio)
n_components = np.argmax(cumulative_variance >= 0.95) + 1

principal_components_train = np.dot(data_train, eigenvectors[:, :n_components])

principal_df_train = pd.DataFrame(
    principal_components_train,
    columns=[f"PC{i+1}" for i in range(n_components)]
)
final_train_df = pd.concat([principal_df_train, train_df['Result'].reset_index(drop=True)], axis=1)

numeric_df_test = test_df.select_dtypes(include=['number'])
data_test = numeric_df_test.to_numpy()
principal_components_test = np.dot(data_test, eigenvectors[:, :n_components])
principal_df_test = pd.DataFrame(
    principal_components_test,
    columns=[f"PC{i+1}" for i in range(n_components)]
)
final_test_df = pd.concat([principal_df_test, test_df['Result'].reset_index(drop=True)], axis=1)

# Print results
print("Explained Variance Ratio (Training Data):", explained_variance_ratio[:n_components])
print("Number of Principal Components:", n_components)
print(final_train_df.head())
print(final_test_df.head())

"""Here we have done PCA to get the principal components and project our already present data on these components to receive our new data frame.

# logistic
"""

import numpy as np
import matplotlib.pyplot as plt

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_cost(X, y, weights):
    m = len(y)
    h = sigmoid(X @ weights)
    epsilon = 1e-5
    cost = -(1 / m) * (y.T @ np.log(h + epsilon) + (1 - y).T @ np.log(1 - h + epsilon))
    return cost

def gradient_descent(X, y, weights, learning_rate, iterations):
    m = len(y)
    cost_history = []

    for _ in range(iterations):
        predictions = sigmoid(X @ weights)
        gradient = (1 / m) * X.T @ (predictions - y)
        weights -= learning_rate * gradient
        cost = compute_cost(X, y, weights)
        cost_history.append(cost)

    return weights, cost_history

def predict(X, weights):
    probabilities = sigmoid(X @ weights)
    return (probabilities >= 0.5).astype(int)

def logistic_regression_manual(X, y, learning_rate=0.05, iterations=1000):
    X = np.c_[np.ones((X.shape[0], 1)), X]
    weights = np.zeros((X.shape[1], 1))
    y = y.reshape(-1, 1)
    weights, cost_history = gradient_descent(X, y, weights, learning_rate, iterations)
    return weights, cost_history

X_train = np.array(principal_components_train)
y_train = np.array(train_df['Result'])

X_test = np.array(principal_components_test)
y_test = np.array(test_df['Result'])

weights, cost_history = logistic_regression_manual(X_train, y_train, learning_rate=0.01, iterations=1000)

X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]
y_pred = predict(X_test_bias, weights)

accuracy = np.mean(y_pred.flatten() == y_test) * 100
print(f"Accuracy: {accuracy:.2f}%")

flat_cost_history = np.array(cost_history).flatten()

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def compute_cost_l1(X, y, weights, lambda_):
    m = len(y)
    h = sigmoid(X @ weights)
    epsilon = 1e-5
    cost = -(1 / m) * (y.T @ np.log(h + epsilon) + (1 - y).T @ np.log(1 - h + epsilon))
    l1_penalty = lambda_ * np.sum(np.abs(weights[1:]))
    return cost + l1_penalty

def gradient_descent_l1(X, y, weights, learning_rate, iterations, lambda_):
    m = len(y)
    cost_history = []

    for _ in range(iterations):
        predictions = sigmoid(X @ weights)
        gradient = (1 / m) * X.T @ (predictions - y)
        l1_gradient = lambda_ * np.sign(weights)
        l1_gradient[0] = 0
        weights -= learning_rate * (gradient + l1_gradient)
        cost = compute_cost_l1(X, y, weights, lambda_)
        cost_history.append(cost)

    return weights, cost_history

def predict(X, weights):
    probabilities = sigmoid(X @ weights)
    return (probabilities >= 0.5).astype(int)

def logistic_regression_l1(X, y, learning_rate=0.01, iterations=1000, lambda_=0.01):
    X = np.c_[np.ones((X.shape[0], 1)), X]
    weights = np.zeros((X.shape[1], 1))
    y = y.reshape(-1, 1)
    weights, cost_history = gradient_descent_l1(X, y, weights, learning_rate, iterations, lambda_)
    return weights, cost_history

X_train = np.array(principal_components_train)
y_train = np.array(train_df['Result'])

X_test = np.array(principal_components_test)
y_test = np.array(test_df['Result'])

weights, cost_history = logistic_regression_l1(X_train, y_train, learning_rate=0.01, iterations=1000, lambda_=0.009)

X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]
y_pred = predict(X_test_bias, weights)

accuracy = np.mean(y_pred.flatten() == y_test) * 100
print(f"Accuracy: {accuracy:.2f}%")

"""# SVM"""

"""# DECISION TREE"""

base_dir = "/home/prabhav/SPE_Project/backend"
model_dir = os.path.join(base_dir, "models")
os.makedirs(model_dir, exist_ok=True)

# Ensure consistent feature names (must match API)
numerical_features_api = [
    'age',
    'gender',
    'total_bilirubin', 
    'direct_bilirubin',
    'alkaline_phosphotase',
    'alanine_aminotransferase',
    'aspartate_aminotransferase',
    'total_proteins',
    'albumin',
    'albumin_globulin_ratio'
]

# Convert back to DataFrame with correct column names
X_train_df = pd.DataFrame(principal_components_train, 
                         columns=[f"PC{i+1}" for i in range(n_components)])
X_test_df = pd.DataFrame(principal_components_test,
                       columns=[f"PC{i+1}" for i in range(n_components)])

# Save model with all required components
model_path = os.path.join(model_dir, "logistic_model.pkl")
with open(model_path, 'wb') as f:
    pickle.dump({
        'weights': weights,
        'mean': train_mean.values,  # Save as numpy array
        'std': train_std.values,    # Save as numpy array
        'eigenvectors': eigenvectors[:len(numerical_features), :n_components],  # Correct dimensions
        'n_components': n_components,
        'feature_names': numerical_features_api,  # API-compatible names
        'original_feature_names': numerical_features  # Original names from dataset
    }, f)
print("âœ… Model saved to:", model_path)